{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f407d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f5fc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_missed_payments</th>\n",
       "      <th>number_of_bank_visits</th>\n",
       "      <th>score</th>\n",
       "      <th>income</th>\n",
       "      <th>use_online_streaming</th>\n",
       "      <th>number_of_bank_accounts</th>\n",
       "      <th>state</th>\n",
       "      <th>bad_customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16601.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>16500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_missed_payments  number_of_bank_visits  score   income  \\\n",
       "0                        4.0                    NaN    NaN  16601.0   \n",
       "1                        0.0                    NaN    NaN  42800.0   \n",
       "2                       11.0                   30.0  607.0  16500.0   \n",
       "3                       29.0                    NaN  578.0      NaN   \n",
       "4                        0.0                    NaN    NaN      NaN   \n",
       "\n",
       "   use_online_streaming  number_of_bank_accounts        state  bad_customer  \n",
       "0                   0.0                      5.0  West Bengal           1.0  \n",
       "1                   0.0                      1.0  Maharashtra           1.0  \n",
       "2                   0.0                      1.0   Tamil Nadu           0.0  \n",
       "3                   0.0                      3.0  Maharashtra           0.0  \n",
       "4                   0.0                      0.0    Telangana           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/pankajkalania/IV-WOE/main/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527fec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   number_of_missed_payments  972 non-null    float64\n",
      " 1   number_of_bank_visits      678 non-null    float64\n",
      " 2   score                      774 non-null    float64\n",
      " 3   income                     874 non-null    float64\n",
      " 4   use_online_streaming       1000 non-null   float64\n",
      " 5   number_of_bank_accounts    972 non-null    float64\n",
      " 6   state                      999 non-null    object \n",
      " 7   bad_customer               1000 non-null   float64\n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8306b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check monotonicity of a series\n",
    "def is_monotonic(temp_series):\n",
    "    return all(temp_series[i] <= temp_series[i + 1] for i in range(len(temp_series) - 1)) or all(temp_series[i] >= temp_series[i + 1] for i in range(len(temp_series) - 1))\n",
    "\n",
    "def prepare_bins(bin_data, c_i, target_col, max_bins):\n",
    "    force_bin = True\n",
    "    binned = False\n",
    "    remarks = np.nan\n",
    "    # ----------------- Monotonic binning -----------------\n",
    "    for n_bins in range(max_bins, 2, -1):\n",
    "        try:\n",
    "            bin_data[c_i + \"_bins\"] = pd.qcut(bin_data[c_i], n_bins, duplicates=\"drop\")\n",
    "            monotonic_series = bin_data.groupby(c_i + \"_bins\")[target_col].mean().reset_index(drop=True)\n",
    "            if is_monotonic(monotonic_series):\n",
    "                force_bin = False\n",
    "                binned = True\n",
    "                remarks = \"binned monotonically\"\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    # ----------------- Force binning -----------------\n",
    "    # creating 2 bins forcefully because 2 bins will always be monotonic\n",
    "    if force_bin or (c_i + \"_bins\" in bin_data and bin_data[c_i + \"_bins\"].nunique() < 2):\n",
    "        _min=bin_data[c_i].min()\n",
    "        _mean=bin_data[c_i].mean()\n",
    "        _max=bin_data[c_i].max()\n",
    "        bin_data[c_i + \"_bins\"] = pd.cut(bin_data[c_i], [_min, _mean, _max], include_lowest=True)\n",
    "        if bin_data[c_i + \"_bins\"].nunique() == 2:\n",
    "            binned = True\n",
    "            remarks = \"binned forcefully\"\n",
    "    \n",
    "    if binned:\n",
    "        return c_i + \"_bins\", remarks, bin_data[[c_i, c_i+\"_bins\", target_col]].copy()\n",
    "    else:\n",
    "        remarks = \"couldn't bin\"\n",
    "        return c_i, remarks, bin_data[[c_i, target_col]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9f93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate WOE and IV for every group/bin/class for a provided feature\n",
    "def iv_woe_4iter(binned_data, target_col, class_col):\n",
    "    if \"_bins\" in class_col:\n",
    "        binned_data[class_col] = binned_data[class_col].cat.add_categories(['Missing'])\n",
    "        binned_data[class_col] = binned_data[class_col].fillna(\"Missing\")\n",
    "        temp_groupby = binned_data.groupby(class_col).agg({class_col.replace(\"_bins\", \"\"):[\"min\", \"max\"],\n",
    "                                                           target_col: [\"count\", \"sum\", \"mean\"]}).reset_index()\n",
    "    else:\n",
    "        binned_data[class_col] = binned_data[class_col].fillna(\"Missing\")\n",
    "        temp_groupby = binned_data.groupby(class_col).agg({class_col:[\"first\", \"first\"],\n",
    "                                                           target_col: [\"count\", \"sum\", \"mean\"]}).reset_index()\n",
    "    \n",
    "    temp_groupby.columns = [\"sample_class\", \"min_value\", \"max_value\", \"sample_count\", \"event_count\", \"event_rate\"]\n",
    "    temp_groupby[\"non_event_count\"] = temp_groupby[\"sample_count\"] - temp_groupby[\"event_count\"]\n",
    "    temp_groupby[\"non_event_rate\"] = 1 - temp_groupby[\"event_rate\"]\n",
    "    temp_groupby = temp_groupby[[\"sample_class\", \"min_value\", \"max_value\", \"sample_count\",\n",
    "                                 \"non_event_count\", \"non_event_rate\", \"event_count\", \"event_rate\"]]\n",
    "    \n",
    "    if \"_bins\" not in class_col and \"Missing\" in temp_groupby[\"min_value\"]:\n",
    "        temp_groupby[\"min_value\"] = temp_groupby[\"min_value\"].replace({\"Missing\": np.nan})\n",
    "        temp_groupby[\"max_value\"] = temp_groupby[\"max_value\"].replace({\"Missing\": np.nan})\n",
    "    temp_groupby[\"feature\"] = class_col\n",
    "    if \"_bins\" in class_col:\n",
    "        temp_groupby[\"sample_class_label\"]=temp_groupby[\"sample_class\"].replace({\"Missing\": np.nan}).astype('category').cat.codes.replace({-1: np.nan})\n",
    "    else:\n",
    "        temp_groupby[\"sample_class_label\"]=np.nan\n",
    "    temp_groupby = temp_groupby[[\"feature\", \"sample_class\", \"sample_class_label\", \"sample_count\", \"min_value\", \"max_value\",\n",
    "                                 \"non_event_count\", \"non_event_rate\", \"event_count\", \"event_rate\"]]\n",
    "    \n",
    "    \"\"\"\n",
    "    **********get distribution of good and bad\n",
    "    \"\"\"\n",
    "    temp_groupby['distbn_non_event'] = temp_groupby[\"non_event_count\"]/temp_groupby[\"non_event_count\"].sum()\n",
    "    temp_groupby['distbn_event'] = temp_groupby[\"event_count\"]/temp_groupby[\"event_count\"].sum()\n",
    "\n",
    "    temp_groupby['woe'] = np.log(temp_groupby['distbn_non_event'] / temp_groupby['distbn_event'])\n",
    "    temp_groupby['iv'] = (temp_groupby['distbn_non_event'] - temp_groupby['distbn_event']) * temp_groupby['woe']\n",
    "    \n",
    "    temp_groupby[\"woe\"] = temp_groupby[\"woe\"].replace([np.inf,-np.inf],0)\n",
    "    temp_groupby[\"iv\"] = temp_groupby[\"iv\"].replace([np.inf,-np.inf],0)\n",
    "    \n",
    "    return temp_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab31a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "- iterate over all features.\n",
    "- calculate WOE & IV for there classes.\n",
    "- append to one DataFrame woe_iv.\n",
    "\"\"\"\n",
    "def var_iter(data, target_col, max_bins):\n",
    "    woe_iv = pd.DataFrame()\n",
    "    remarks_list = []\n",
    "    for c_i in data.columns:\n",
    "        if c_i not in [target_col]:\n",
    "            # check if binning is required. if yes, then prepare bins and calculate woe and iv.\n",
    "            \"\"\"\n",
    "            ----logic---\n",
    "            binning is done only when feature is continuous and non-binary.\n",
    "            Note: Make sure dtype of continuous columns in dataframe is not object.\n",
    "            \"\"\"\n",
    "            c_i_start_time=time.time()\n",
    "            if np.issubdtype(data[c_i], np.number) and data[c_i].nunique() > 2:\n",
    "                class_col, remarks, binned_data = prepare_bins(data[[c_i, target_col]].copy(), c_i, target_col, max_bins)\n",
    "                agg_data = iv_woe_4iter(binned_data.copy(), target_col, class_col)\n",
    "                remarks_list.append({\"feature\": c_i, \"remarks\": remarks})\n",
    "            else:\n",
    "                agg_data = iv_woe_4iter(data[[c_i, target_col]].copy(), target_col, c_i)\n",
    "                remarks_list.append({\"feature\": c_i, \"remarks\": \"categorical\"})\n",
    "            # print(\"---{} seconds. c_i: {}----\".format(round(time.time() - c_i_start_time, 2), c_i))\n",
    "            woe_iv = woe_iv.append(agg_data)\n",
    "    return woe_iv, pd.DataFrame(remarks_list)\n",
    "\n",
    "# after getting woe and iv for all classes of features calculate aggregated IV values for features.\n",
    "def get_iv_woe(data, target_col, max_bins):\n",
    "    func_start_time = time.time()\n",
    "    woe_iv, binning_remarks = var_iter(data, target_col, max_bins)\n",
    "    print(\"------------------IV and WOE calculated for individual groups.------------------\")\n",
    "    print(\"Total time elapsed: {} minutes\".format(round((time.time() - func_start_time) / 60, 3)))\n",
    "    \n",
    "    woe_iv[\"feature\"] = woe_iv[\"feature\"].replace(\"_bins\", \"\", regex=True)    \n",
    "    woe_iv = woe_iv[[\"feature\", \"sample_class\", \"sample_class_label\", \"sample_count\", \"min_value\", \"max_value\",\n",
    "                     \"non_event_count\", \"non_event_rate\", \"event_count\", \"event_rate\", 'distbn_non_event',\n",
    "                     'distbn_event', 'woe', 'iv']]\n",
    "    \n",
    "    iv = woe_iv.groupby(\"feature\")[[\"iv\"]].agg([\"sum\", \"count\"]).reset_index()\n",
    "    print(\"------------------Aggregated IV values for features calculated.------------------\")\n",
    "    print(\"Total time elapsed: {} minutes\".format(round((time.time() - func_start_time) / 60, 3)))\n",
    "    \n",
    "    iv.columns = [\"feature\", \"iv\", \"number_of_classes\"]\n",
    "    null_percent_data=pd.DataFrame(data.isnull().mean()).reset_index()\n",
    "    null_percent_data.columns=[\"feature\", \"feature_null_percent\"]\n",
    "    iv=iv.merge(null_percent_data, on=\"feature\", how=\"left\")\n",
    "    print(\"------------------Null percent calculated in features.------------------\")\n",
    "    print(\"Total time elapsed: {} minutes\".format(round((time.time() - func_start_time) / 60, 3)))\n",
    "    iv = iv.merge(binning_remarks, on=\"feature\", how=\"left\")\n",
    "    woe_iv = woe_iv.merge(iv[[\"feature\", \"iv\", \"remarks\"]].rename(columns={\"iv\": \"iv_sum\"}), on=\"feature\", how=\"left\")\n",
    "    print(\"------------------Binning remarks added and process is complete.------------------\")\n",
    "    print(\"Total time elapsed: {} minutes\".format(round((time.time() - func_start_time) / 60, 3)))\n",
    "    return iv, woe_iv.replace({\"Missing\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b59cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
